
Prends en paramètre un fichier, qui est au format Markdown.
Dans ce fichier, il y a des images, présentes chacune comme ceci : ![](data:image/x-emf;base64,AQAAA...==)
Tu dois invoquer le modèle gpt-5-nano en web-service chez OpenAI, en utilisant la clé d'accès stockée en variable d'environnement nommée OPENAIAPIKEY, pour lui fournir l'image et lui demander une version textuelle avec le prompt suivant :
"Voici une image, fournis-moi un texte en Markdown qui décrit son contenu pour pouvoir l'inclure dans un chunk d'un système d'IA générative de type RAG".
Prends la réponse fournie par le modèle et remplace l'image par ce texte.
Fais cela en boucle pour toutes les images du fichier d'origine.
Produis un fichier de sortie nommé comme le fichier d'entrée, en rajoutant le suffixe "-converted.md".

-----------------

Avant de les envoyer à OpenAI, convertit les images en format png.

-----------------

Fais en sorte que cette conversion fonctionne aussi pour le format image/x-emf.

------------------

Rajoute une option de ligne de commande "-l" (comme "local") qui, lorsqu'elle est sélectionnée, invoque non pas OpenAI mais Ollama qui tourne en local, en demandant le modèle gpt-oss:20b

------------------

Rajoute un cache disque de la transformation en texte d'une image, comme ceci :
- pour chaque image incluse dans le fichier d'entrée, on calcule le hash SHA-1 des données base64 représentant l'image, et on tranforme ce hash en une chaîne hexadécimale pour pouvoir l'inclure dans des noms de fichiers.
- pour chaque image incluse dans le fichier d'entrée, le texte décrivant le contenu de l'image est sauvegardé dans un fichier nommé à partir du nom du fichier d'entrée, auquel on rajoute le suffixe ".cache-" suivi de la chaîne hexadécimale du hash, suivi encore de ".txt".
- à chaque fois qu'on veut interroger un LLM pour avoir la description d'une image, on vérifie que le fichier de cache existe, et si c'est le cas, on n'interroge pas le LLM mais on utilise son contenu. Quand il n'existe pas, on interroge le LLM, on utilise son contenu et on le place aussi dans le cache en créant le fichier correspondant.

------------------

Rajoute une option de ligne de commande "-t" (comme "text") qui, lorsqu'elle est sélectionnée, n'invoque aucun llm, ne cherche pas de cache, ne met rien en cache, mais se contente de considérer que le texte qui décrit l'image est "pas de description", et le rajoute donc dans la sortie comme s'il provenait du cache ou d'un llm.
