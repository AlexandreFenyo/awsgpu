{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOzDjamH/G2PY01sUvwXCXK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexandreFenyo/awsgpu/blob/main/notebooks/RAG_llamaindex_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG avec Llamaindex\n",
        "\n",
        "Je suis le plan d'action fourni par GPT-5-nano : \"Je veux créer un notebook dans Google Colab, pour utiliser Llamaindex afin de procéder à de la recherche de type RAG. Je ne veux pas invoquer de LLM ensuite, je me limite à récupérer des chunks via du RAG.\"\n",
        "\n",
        "J'installe les biblothèques utiles :"
      ],
      "metadata": {
        "id": "uunuUErOkiLK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-cQICTOgMS2"
      },
      "outputs": [],
      "source": [
        "!pip -q install --upgrade llama-index llama-index-embeddings-huggingface transformers sentence-transformers # faiss-cpu"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importations Python pour les embeddings"
      ],
      "metadata": {
        "id": "MOkcbKKmk4eB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core.node_parser import TokenTextSplitter\n",
        "from llama_index.core import Settings"
      ],
      "metadata": {
        "id": "B8PsDxQak8rB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Monter Drive et pointer vers les documents"
      ],
      "metadata": {
        "id": "2EVWiocgnbHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive', force_remount=True)\n",
        "docs_dir = \"/content/drive/MyDrive/RAG_docs\"\n",
        "docs_dir_path = Path(docs_dir)\n",
        "print(\"Répertoire des documents : \", docs_dir_path, \"\\nMontage réalisé :\", docs_dir_path.exists())\n",
        "docs = SimpleDirectoryReader(input_dir=str(docs_dir_path)).load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgfblDrvndZK",
        "outputId": "21307472-9217-4950-fab7-dcf94b1df8ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Répertoire des documents :  /content/drive/MyDrive/RAG_docs \n",
            "Montage réalisé : True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Travailler avec des Embeddings locaux"
      ],
      "metadata": {
        "id": "iqMft72Gmofq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "Settings.embed_model = embedding_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAADUvRhmskS",
        "outputId": "8d7b4320-3b8b-4ae8-94d4-a8ba5ab4039d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Construire l'index avec VectorStoreIndex.from_documents\n",
        "\n",
        "Ce que fait VectorStoreIndex.from_documents :\n",
        "- Convertit tes documents en chunks (selon le TextSplitter que tu utilises), calcule les embeddings et les stocke dans un « vector store » (un index de vecteurs).\n",
        "- Côté récupération, il interroge ce même vector store pour trouver les chunks les plus pertinents par similarity.\n",
        "- Tu n’es pas obligé d’avoir une base externe (Weaviate, Pinecone, Qdrant, etc.). Si tu ne précises pas de vector store, llama-index utilise par défaut un stockage vectoriel en mémoire (un FAISS-like store) et peut persister sur disque via StorageContext. (developers.llamaindex.ai)\n"
      ],
      "metadata": {
        "id": "Lek5eViunB2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorStoreIndex.from_documents(docs, embed_model=embedding_model)"
      ],
      "metadata": {
        "id": "XQMZtMb4mrL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper pour l'affichage des chunks selon leurs types respectifs"
      ],
      "metadata": {
        "id": "gLYSCAhcsKi7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text(obj):\n",
        "  if hasattr(obj, \"text\"):\n",
        "    return obj.text\n",
        "  if hasattr(obj, \"node\"):\n",
        "    node = obj.node\n",
        "  if hasattr(node, \"text\"):\n",
        "    return node.text\n",
        "  if hasattr(node, \"get_text\"):\n",
        "    return node.get_text()\n",
        "  return str(obj)"
      ],
      "metadata": {
        "id": "V8K3-UZDsQDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Récupération de chunks via une requête"
      ],
      "metadata": {
        "id": "wYMJC1jtrRHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = index.as_retriever(search_kwargs={\"k\": 5})\n",
        "retrieved_nodes = retriever.retrieve(\"ta requête\")"
      ],
      "metadata": {
        "id": "i9KkjC50rU95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Affichage des chunks récupérés"
      ],
      "metadata": {
        "id": "2LBUh22nskMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, n in enumerate(retrieved_nodes, start=1):\n",
        "  print(f\"--- Chunk {i} ---\")\n",
        "  print(extract_text(n))\n",
        "  print()"
      ],
      "metadata": {
        "id": "I-2RkPcpsmh6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}